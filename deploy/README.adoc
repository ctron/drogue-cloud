:icons: font

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

:toc:
:toc-placement!:

= Deploying

This guide should take your through the process of deploying the example.

NOTE: All commands are relative to the root of the repository.

'''

toc::[]

== Pre-requisites

The following pre-requisites are required to run this. As there are many ways to obtain them, I will stick to my
own environment, and try to point out possible differences.

Also, there may be newer versions of the components I used, maybe you can upgrade, maybe not.

=== Command line tools

The usual command line tools. I would like to call out the following:

* `tkn` - The Tekton command line client. You can ignore it if you don't want to use Tekton.
* `kn` - The Knative command line tool. Not required, but might come in handy.
* `oc` / `kubectl` - In our case, there isn't a big difference. I try to go for `kubectl`, but should
you encounter `oc`, then just swap it. And/or raise a PR ðŸ˜‰
* `http` - aka https://httpie.org/[HTTPie]
* `mqtt` - aka https://github.com/hivemq/mqtt-cli[MQTT CLI]

=== Kubernetes

You will need an installation of Kubernetes. I am using OpenShift 4.5, but you should be able to take any other
Kubernetes 1.17+.

The biggest differences from OpenShift to Kubernetes may be:
* The lack of the `Route` CRD. Which exposes services easily, without requiring any load balancer or other tunnel
  workarounds.
* The lack of an internal image registry. Which you can ignore if you build your images manually, or have other options.

Although I am using a dedicated cluster, using
https://developers.redhat.com/products/codeready-containers/overview[CRC]footnote:[CodeReady Containers, OpenShift in a local VM]
should be possible as well. Just ensure that you bring enough CPU and RAM.

=== Knative 0.13.x

On the cluster, you will need https://knative.dev/[Knative], including the following features:

* Serving
* Eventing
* Kafka event support

I walked through the steps of "OpenShift Serverless Operator 1.7.x" (https://access.redhat.com/documentation/en-us/openshift_container_platform/4.5/html/serverless_applications/installing-openshift-serverless-1),
installed via OperatorHub.

I needed to add the Kafka eventing after that, by partially following this guide: https://openshift-knative.github.io/docs/docs/proc_apache-kafka.html.
You can install the operator following these instructions, but don't create any CRfootnote:[custom resource] yet.

=== Strimzi

For Kafka, this deployment requires https://strimzi.io/[Strimzi]. I deployed "AMQ Streams" from OperatorHub.

=== Tekton Pipelines 0.11 (optional)

As I wanted to build images on my cluster as well, I added a https://tekton.dev[Tekton] pipeline as well.
You can skip this step, but then you need to build images yourself, and push them somewhere the deployment
can pull them from, later on.

I installed "OpenShift Pipelines Operator" via OperatorHub.

== Performing the deployment

Once you have all pre-requisites installed, you can start deploying.

=== Create a new namespace

----
oc new-project drogue-iot
oc label namespace drogue-iot bindings.knative.dev/include=true
----

Or:

----
kubectl create namespace drogue-iot
kubectl label namespace drogue-iot bindings.knative.dev/include=true
kubectl config set-context --current --namespace=drogue-iot
----

NOTE: Unless noted otherwise, the guide will expect that your context defaults to the `drogue-iot` namespace.

WARNING: Currently, some parts of the deployment actually expect the name `drogue-iot`. If you change it, you will
break things.

=== Build with Tekton

NOTE: For a local build, see the next section.

==== Deploy the pipeline

To deploy the pipeline, run:

    kubectl apply -f deploy/01-build

NOTE: If you don't have OpenShift, ignore the failure on `010-ImageStreams.yaml`.

==== Create a workspace claim

You will need a workspace for running the build pipeline. It is expected to be a PVCfootnote:[persistent volume claim]]
with the name of `build-workspace`.

You can create one with the following command:

    kubectl apply -f deploy/build-pvc.yaml

The claim can be reused for builds. Of course, it can also be destroyed and re-created.

==== Trigger a build

----
tkn pipeline start build-drogue-cloud --showlog -p repo-owner=drogue-iot -p ref-name=main --workspace name=shared-data,claimName=build-workspace
----

[NOTE]
====
.Git ref/commit/tag/branch

By default, the pipeline will build the `main` branch. You can use `-p ref-name=<commit|tag|branch>` to select
a specific Git commit, tag, or branch. Best synchronize this with the version of the documentation you are using.
====

Wait for the build to succeed.

[NOTE]
====
.GitHub repository

The previous build assumes you re-use this exact repository to perform the build. Of course, you can also fork
the repository and use `-p repo-owner=your-user` in the previous command.
====

[NOTE]
====
.Internal image registry

By default, this pushes to the OpenShift internal registry. You can override the target registry using
`-p image-registry=my-target`. However, you probably will need to attach push credentials using tekton as well.
====

=== Local build

If you don't want to use build pipelines, you can build the images locally:

----
make CONTAINER_REGISTRY=quay.io/my-user
----

This will push the following images, be sure to grant access to them (aka make them public):

* `http-endpoint`
* `mqtt-endpoint`
* `influxdb-pusher`

==== Building docker images using containers (cross compilation)

If rust is not installed on your system or if you use OSX you can build the image inside a docker container using the multi stage Dockerfile : 

----
make CONTAINER_REGISTRY=quay.io/my-user docker-rust-build
----

Don't forget to push your images : 

----
make CONTAINER_REGISTRY=quay.io/my-user push
----


==== Building the rust artifacts in a container

If rust is not installed on your system or if you use OSX but you want to build the rust artifact locally you can use the following docker command : 

----
docker run \
--rm \
-it \
--volume ${PWD}:/opt/volume:z \
--workdir /opt/volume \
rust:latest \
cargo build --release
----

Keep in mind that while you have built the rust binaries, there are not bundled into a docker image. You can build and push with them with : 

----
make CONTAINER_REGISTRY=quay.io/my-user build-docker-images
make CONTAINER_REGISTRY=quay.io/my-user push   
----

=== Deploy Kafka

This deploys a Kafka cluster and creates the Kafka channel capability.

[NOTE]
====
The following command operates on the namespace `knative-eventing`. The namespace is specified in the YAML file.
So be careful here when use `kubectl` with `-n`.
====

----
kubectl apply -f deploy/02-deploy/01-kafka
----

=== Deploy InfluxDB

Execute the following:

----
kubectl apply -f deploy/02-deploy/02-influxdb
----

=== Deploy Grafana

Execute the following:

----
kubectl apply -f deploy/02-deploy/03-dashboard
----

Credentials: `admin` / `admin123456`

=== Deploy Knative services

Depending on your environment, you need to fix the source of your images. Check the files,
and adapt the `image` field:

* `deploy/02-deploy/04-knative/110-Service-influxdb-pusher.yaml`
* `deploy/02-deploy/05-endpoints/http/020-Service-http-endpoint.yaml`
* `deploy/02-deploy/05-endpoints/mqtt/010-Deployment-mqtt-endpoint.yaml`

You must also provide a TLS certificate for the MQTT endpoint. You can create a secret like this:

----
kubectl create secret tls mqtt-endpoint-tls --key server.key --cert fullchain.pem
----

Also see: link:02-deploy/05-endpoints/mqtt/010-Secret-mqtt.yaml[02-deploy/05-endpoints/mqtt/010-Secret-mqtt.yaml]

Then execute the following:

----
kubectl apply -f deploy/02-deploy/04-knative
kubectl apply -f deploy/02-deploy/05-endpoints/http
kubectl apply -f deploy/02-deploy/05-endpoints/mqtt
----

== Testing

=== Endpoints

First you need to figure out the endpoints for the Grafana instance and the HTTP adapter.

If you are running on OpenShift, and have properly configured the ingress operator, then this should be easy.

The following command should give you the endpoint of the Grafana instance and the MQTT endpoint:
----
oc get routes
----

While the next command gives you the URL of the HTTP endpoint:

----
kn route describe http-endpoint
----

NOTE: It may be that the endpoint of the Knative service is listed as `http:`, while in fact it is `https:`.

NOTE: If you are not running on OpenShift, you may need some extra configuration for exposing services. Please check
with the documentation of your Kubernetes variant.

=== Dashboard

Login in to the Grafana instance and open the dashboard "Knative test". Double check that it is set to
automatically reload.

=== Publish data using HTTP

==== Execute

From the command line run (and be sure to replace the URL with your own):

----
http POST https://http-endpoint-drogue-iot.apps.my.cluster/publish/foo temp:=2.5
----

==== Verify

The result should be something like:

----
HTTP/1.1 202 Accepted
content-length: 0
date: Fri, 11 Sep 2020 12:07:17 GMT
server: envoy
set-cookie: 84c0cd5758bb97f4b5bed57575911131=531e737940bb08052e1fa4cc58c12866; path=/; HttpOnly
x-envoy-upstream-service-time: 3616
----

If the content was accepted, it should pop up in the dashboard after a few seconds.

==== What just happened?!

* The data was published to the HTTP endpoint. Pre-processed and converted into a "cloud event",
* That cloud event was delivered to the Kafka channel, which stores it,
* The InfluxDB pusher got notified from the Kafka source (attached to the Kafka channel of the HTTP endpoint)â€¦
* â€¦ and writes it to the InfluxDB,
* From where the Grafana dashboard will poll it.

Like this:

.Overview diagram
image::../images/example.svg[Overview]

=== Publish data (MQTT)

==== Execute (MQTT v3.1.1)

From the command line run (and be sure to replace the host and port with your own):

----
mqtt pub -h mqtt-endpoint-drogue-iot.apps.my.cluster -p 443 -s -t temp -m '{"temp":33}' -V 3
----

==== Execute (MQTT v5)

From the command line run (and be sure to replace the host and port with your own):

----
mqtt pub -h mqtt-endpoint-drogue-iot.apps.my.cluster -p 443 -s -t temp -m '{"temp":33}'
----

==== Verify

In the output, you should see something like `received PUBLISH acknowledgement` as one of the
last messages. If the content was accepted, it should pop up in the dashboard after a few seconds.

==== What just happened?!

* The data was published to the MQTT endpoint. Pre-processed and converted into a "cloud event",
* That cloud event was delivered to the Kafka channel, which stores it,
* The InfluxDB pusher got notified from the Kafka source (attached to the Kafka channel of the HTTP endpoint)â€¦
* â€¦ and writes it to the InfluxDB,
* From where the Grafana dashboard will poll it.
